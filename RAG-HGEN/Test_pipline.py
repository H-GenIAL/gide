import tiktoken
import boto3
import json
import faiss
import requests
import numpy as np

# DÃ©finir le chemin du fichier Markdown
md_file = "Bail StockWay.md"

# Lire tout le contenu dans une variable string
with open(md_file, "r", encoding="utf-8") as file:
    md_content = file.read()

print(md_content[:400])  # Affiche le contenu du fichier dans la console

# DÃ©finir la fonction de dÃ©coupage
def split_text(text, chunk_size=500, overlap=100):
    encoding = tiktoken.get_encoding("cl100k_base")
    tokens = encoding.encode(text)
    
    chunks = []
    for i in range(0, len(tokens), chunk_size - overlap):
        chunk = tokens[i : i + chunk_size]
        chunks.append(encoding.decode(chunk))
    
    return chunks

# DÃ©couper le texte
chunks = split_text(md_content, chunk_size=500, overlap=100)
print(f"{len(chunks)} chunks gÃ©nÃ©rÃ©s.")
print(len(md_content))


bedrock_runtime = boto3.client("bedrock-runtime", region_name="us-west-2")

vectors = []
for chunk in chunks:
    response = bedrock_runtime.invoke_model(
        modelId="amazon.titan-embed-text-v2:0",
        contentType="application/json",
        accept="application/json",
        body=json.dumps({"inputText": chunk})
    )
    embedding = json.loads(response["body"].read())["embedding"]
    vectors.append(embedding)

# Convertir la liste de listes en un tableau NumPy 2D
vectors = np.array(vectors, dtype=np.float32)

# VÃ©rifier la forme du tableau (nombre de vecteurs, dimension des embeddings)
print("Shape of vectors array:", vectors.shape)

print("Vectorisation terminÃ©e !")


for i, vector in enumerate(vectors):
    document = {
        "text": chunks[i],
        "vector": vector
    }

# ğŸ“Œ **CrÃ©er l'index FAISS**
dimension = vectors.shape[1]  # Dimension des embeddings
index = faiss.IndexFlatL2(dimension)  # Index de type L2 (distance Euclidienne)

# **ğŸ”´ Ã‰viter la duplication des vecteurs en rÃ©initialisant l'index**
index.reset()  # RÃ©initialise FAISS avant d'ajouter de nouveaux vecteurs

index.add(vectors)  # Ajouter les vecteurs Ã  FAISS

print("Vectorisation et stockage FAISS terminÃ©s !")

# ğŸ” **Rechercher les passages les plus pertinents**
query = "Quel est le nom du bailleur ?"

# Transformer la question en vecteur
query_response = bedrock_runtime.invoke_model(
    modelId="amazon.titan-embed-text-v2:0",
    contentType="application/json",
    accept="application/json",
    body=json.dumps({"inputText": query})
)
query_vector = json.loads(query_response["body"].read())["embedding"]

query_vector = np.array(query_vector, dtype=np.float32).reshape(1, -1)

# ğŸ“Œ **Effectuer la recherche KNN**
k = 3  # ğŸ”´ Modifier selon le nombre de rÃ©sultats souhaitÃ©s
distances, indices = index.search(query_vector, k)

# ğŸ“œ **RÃ©cupÃ©rer les passages les plus pertinents**
retrieved_texts = [chunks[i] for i in indices[0]]

print("\nğŸ” Passages les plus pertinents trouvÃ©s :")
for text in retrieved_texts:
    print("\nâœ", text)

# ğŸ“ **Construire le contexte pour la gÃ©nÃ©ration de rÃ©ponse**
context = " ".join(retrieved_texts)

prompt = f"En utilisant les informations suivantes, rÃ©ponds Ã  cette question :\n\n{context}\n\nQuestion: {query}\nRÃ©ponse:"

# **(Facultatif) Utiliser un modÃ¨le LLM pour gÃ©nÃ©rer une rÃ©ponse**
print("\nğŸ“Œ Voici le prompt utilisÃ© pour la gÃ©nÃ©ration de rÃ©ponse :")
print(prompt)


response = bedrock_runtime.invoke_model(
    modelId="mistral.mistral-7b-instruct-v0:2",
    contentType="application/json",
    accept="application/json",
    body=json.dumps({"prompt": prompt, "max_tokens": 300})
)

rag_response = json.loads(response["body"].read())
print("RÃ©ponse augmentÃ©e :", rag_response)
