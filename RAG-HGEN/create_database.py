import os
import json
import numpy as np
import faiss  # Ajout de FAISS pour stockage des vecteurs
import boto3
import tiktoken
import nltk
nltk.download("punkt")
from nltk.tokenize import sent_tokenize

class MarkdownVectorizer:
    def __init__(self, model_emb="amazon.titan-embed-text-v2:0", region="us-west-2"):
        self.model_emb = model_emb
        self.bedrock_runtime = boto3.client("bedrock-runtime", region_name=region)
        self.faiss_index_path = "faiss_index.bin"  # Fichier pour stocker FAISS

    def find_first_md_file(self, directory=None):
        """Trouve le premier fichier .md dans le rÃ©pertoire donnÃ© (ou actuel par dÃ©faut)."""
        if directory is None:
            directory = os.getcwd()
        
        for file in os.listdir(directory):
            if file.lower().endswith(".md"):
                return os.path.join(directory, file)
        
        return None  # Aucun fichier Markdown trouvÃ©

    def load_markdown(self, md_file):
        """Charge le contenu d'un fichier Markdown dans une variable string."""
        with open(md_file, "r", encoding="utf-8") as file:
            return file.read()

    def split_text(self, text, chunk_size=500, overlap=100):
        """DÃ©coupe un texte en chunks, d'abord par paragraphes, puis par phrases si nÃ©cessaire."""
        
        encoding = tiktoken.get_encoding("cl100k_base")
        paragraphs = text.split("\n\n")  # SÃ©parer par paragraphes

        chunks = []
        current_chunk = []
        current_length = 0

        for para in paragraphs:
            tokenized_para = encoding.encode(para)
            
            # Si le paragraphe tient dans un chunk, on l'ajoute directement
            if len(tokenized_para) <= chunk_size:
                if current_length + len(tokenized_para) > chunk_size:
                    # Sauvegarde le chunk prÃ©cÃ©dent
                    chunks.append(encoding.decode(sum(current_chunk, [])))
                    current_chunk = []
                    current_length = 0

                current_chunk.append(tokenized_para)
                current_length += len(tokenized_para)
            else:
                # Si le paragraphe est trop long, on le dÃ©coupe en phrases
                sentences = sent_tokenize(para)
                for sentence in sentences:
                    tokenized_sentence = encoding.encode(sentence)
                    sentence_length = len(tokenized_sentence)

                    if current_length + sentence_length > chunk_size:
                        chunks.append(encoding.decode(sum(current_chunk, [])))

                        # Overlap avec les derniers tokens du chunk prÃ©cÃ©dent
                        if chunks and overlap > 0:
                            overlap_tokens = encoding.encode(" ".join(chunks[-1].split()[-overlap:]))
                            current_chunk = [overlap_tokens]
                            current_length = len(overlap_tokens)
                        else:
                            current_chunk = []
                            current_length = 0

                    current_chunk.append(tokenized_sentence)
                    current_length += sentence_length

        if current_chunk:
            chunks.append(encoding.decode(sum(current_chunk, [])))

        return chunks

    def vectorize_chunks(self, chunks):
        """Transforme chaque chunk en vecteur d'embedding via Amazon Bedrock."""
        vectors = []
        
        for chunk in chunks:
            response = self.bedrock_runtime.invoke_model(
                modelId=self.model_emb,
                contentType="application/json",
                accept="application/json",
                body=json.dumps({"inputText": chunk})
            )
            embedding = json.loads(response["body"].read())["embedding"]
            vectors.append(embedding)
        
        # Convertir la liste en tableau NumPy 2D
        return np.array(vectors, dtype=np.float32)

    def reset_faiss_index(self, dimension):
        """Supprime l'ancienne base FAISS et crÃ©e un nouvel index."""
        if os.path.exists(self.faiss_index_path):
            os.remove(self.faiss_index_path)  # ğŸ”´ Supprime l'ancien fichier FAISS
            print("ğŸ—‘ï¸ Ancienne base FAISS supprimÃ©e.")

        index = faiss.IndexFlatL2(dimension)  # Distance Euclidienne
        faiss.write_index(index, self.faiss_index_path)  # Sauvegarde aprÃ¨s reset
        print("ğŸ”„ FAISS index rÃ©initialisÃ© et stockÃ© en dur.")
        return index

    def load_or_create_faiss_index(self, dimension):
        """Charge l'index FAISS s'il existe, sinon en crÃ©e un nouveau."""
        if os.path.exists(self.faiss_index_path):
            index = faiss.read_index(self.faiss_index_path)
            print("âœ… FAISS index chargÃ© depuis le disque.")
        else:
            index = self.reset_faiss_index(dimension)
        return index

    def process(self):
        """ExÃ©cute toutes les Ã©tapes : trouver, charger, dÃ©couper, vectoriser, et stocker dans FAISS."""
        md_file = self.find_first_md_file()
        if not md_file:
            print("âŒ Aucun fichier Markdown trouvÃ© dans le rÃ©pertoire.")
            return None, None

        print(f"ğŸ“„ Fichier Markdown trouvÃ© : {md_file}")

        md_content = self.load_markdown(md_file)
        print(f"ğŸ”¹ AperÃ§u du texte :\n{md_content[:400]}...\n")

        chunks = self.split_text(md_content)
        print(f"âœ… {len(chunks)} chunks gÃ©nÃ©rÃ©s.")

        vectors = self.vectorize_chunks(chunks)
        print("âœ… Vectorisation terminÃ©e !")
        print("ğŸ“Š Shape of vectors array:", vectors.shape)

        # ğŸ”„ Suppression et rÃ©initialisation de FAISS avant de stocker de nouveaux vecteurs
        dimension = vectors.shape[1]
        index = self.reset_faiss_index(dimension)

        # ğŸ“Œ Ajouter les vecteurs dans FAISS et sauvegarder
        index.add(vectors)
        faiss.write_index(index, self.faiss_index_path)
        print("âœ… FAISS index mis Ã  jour et stockÃ© en dur !")

        return chunks, vectors

if __name__ == "__main__":
    vectorizer = MarkdownVectorizer()
    vectorizer.process()